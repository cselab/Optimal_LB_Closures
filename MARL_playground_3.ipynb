{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efd6c7cf350>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import json\n",
    "from functools import partial\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import seaborn as sn\n",
    " \n",
    "from tianshou.utils import WandbLogger\n",
    "from tianshou.data import Batch, Collector, ReplayBuffer, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.policy import BasePolicy, PPOPolicy, PGPolicy, A2CPolicy\n",
    "from tianshou.trainer import OnpolicyTrainer\n",
    "from tianshou.utils.net.common import ActorCritic, Net\n",
    "#from tianshou.utils.net.discrete import Actor, Critic\n",
    "from tianshou.utils.net.continuous import Actor, Critic, ActorProb\n",
    "from tianshou.trainer.utils import gather_info, test_episode\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.wrappers import TimeLimit, RescaleAction, TransformObservation\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "from lib.environments import *\n",
    "from lib.policy import MarlPPOPolicy, IndpPGPolicy\n",
    "from lib.distributions import ElementwiseNormal\n",
    "from lib.models import get_actor_critic\n",
    "from lib.utils import str2bool, Config, dict_to_wandb_table, restrict_to_num_threads\n",
    "from lib.trainer import MyOnpolicyTrainer\n",
    "from lib.models import FcNN, MyFCNNActorProb, MyFCNNCriticProb\n",
    "from lib.models import *\n",
    "\n",
    "#temporary solution for xlb imports\n",
    "sys.path.append(os.path.abspath('/home/pfischer/XLB'))\n",
    "#from my_flows.kolmogorov_2d import Kolmogorov_flow\n",
    "from my_flows.helpers import get_kwargs\n",
    "\n",
    "\n",
    "#from lib.custom_tianshou.my_actors import MyActorProb\n",
    "\n",
    "import wandb\n",
    "wandb.require(\"core\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after class definition on line 28 (1335503177.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 29\u001b[0;36m\u001b[0m\n\u001b[0;31m    def __init__(self):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after class definition on line 28\n"
     ]
    }
   ],
   "source": [
    "class test_network(nn.Module):\n",
    "\n",
    "    def __init__(self, device=\"cpu\", in_channels=9, feature_dim=27, out_channels=1, padding_mode=\"circular\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=feature_dim, kernel_size=1, stride=1,\n",
    "                       padding=0, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=feature_dim, out_channels=feature_dim, kernel_size=1, stride=1,\n",
    "                       padding=0, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=feature_dim, out_channels=out_channels, kernel_size=1, stride=1,\n",
    "                       padding=0, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float, device=self.device)\n",
    "        batch = obs.shape[0]\n",
    "\n",
    "        values = self.model(obs.reshape(batch, -1, 128, 128))\n",
    "        values = values.reshape(batch,128,128)\n",
    "        return values\n",
    "    \n",
    "\n",
    "    class fcnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(fcnn, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(9, 27),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(27, 27),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(27,1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_network(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(9, 27, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(27, 27, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(27, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = test_network()\n",
    "model2 = fcnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 9, 1, 1])\n",
      "torch.Size([27])\n",
      "torch.Size([27, 27, 1, 1])\n",
      "torch.Size([27])\n",
      "torch.Size([1, 27, 1, 1])\n",
      "torch.Size([1])\n",
      "**********************\n",
      "torch.Size([27, 9])\n",
      "torch.Size([27])\n",
      "torch.Size([27, 27])\n",
      "torch.Size([27])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(model.model[0].weight.shape)\n",
    "print(model.model[0].bias.shape)\n",
    "print(model.model[2].weight.shape)\n",
    "print(model.model[2].bias.shape)\n",
    "print(model.model[4].weight.shape)\n",
    "print(model.model[4].bias.shape)\n",
    "\n",
    "print(\"**********************\")\n",
    "\n",
    "print(model2.model[0].weight.shape)\n",
    "print(model2.model[0].bias.shape)\n",
    "print(model2.model[2].weight.shape)\n",
    "print(model2.model[2].bias.shape)\n",
    "print(model2.model[4].weight.shape)\n",
    "print(model2.model[4].bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re=10000, m_prime=4224, T=22.74231693614892, omega=1.9996472117476842\n",
      "Re=10000, m_prime=67584, T=22.74231693614892, omega=1.99437028369627\n",
      "Re=10000, m_prime=4224, T=22.74231693614892, omega=1.9996472117476842\n",
      "Re=10000, m_prime=67584, T=22.74231693614892, omega=1.99437028369627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pfischer/.miniforge3/envs/xlb2/lib/python3.10/site-packages/tianshou/data/collector.py:69: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n",
      "/home/pfischer/.miniforge3/envs/xlb2/lib/python3.10/site-packages/tianshou/data/collector.py:69: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################################\n",
    "####### environments ##################################################################################\n",
    "#######################################################################################################\n",
    "seeds = np.array([102, 348, 270, 106, 71, 188, 20, 121, 214, 330, 87, 372,\n",
    "              99, 359, 151, 130, 149, 308, 257, 343, 413, 293, 385, 191, 276,\n",
    "              160, 313, 21, 252, 235, 344, 42])\n",
    "\n",
    "assert seeds.shape[0] == np.unique(seeds).shape[0]\n",
    "train_seeds = seeds[:29]\n",
    "val_seeds = seeds[29:]\n",
    "#test_seeds = np.array([69, 33, 420])\n",
    "    \n",
    "train_env = KolmogorovEnvironment9(seeds=train_seeds, max_episode_steps=1535, step_factor=1)\n",
    "test_env = KolmogorovEnvironment9(seeds=val_seeds, max_episode_steps=1535, step_factor=1)\n",
    "#######################################################################################################\n",
    "####### Policy ########################################################################################\n",
    "#######################################################################################################\n",
    "assert train_env.observation_space.shape is not None  # for mypy\n",
    "assert train_env.action_space.shape is not None\n",
    "#initialize PPO\n",
    "actor = MyFCNNActorProb2(in_channels=2, device=device).to(device)\n",
    "critic = MyFCNNCriticProb2(in_channels=2, device=device).to(device)\n",
    "optim = torch.optim.Adam(actor.parameters(), lr=3e-4, eps=1e-7)\n",
    "dist = torch.distributions.Normal\n",
    "#dist = ElementwiseNormal\n",
    "\n",
    "policy = MarlPPOPolicy(actor=actor,\n",
    "    critic=critic, \n",
    "    optim=optim,\n",
    "    dist_fn=dist, \n",
    "    action_space=train_env.action_space,\n",
    "    discount_factor=0.9,\n",
    "    reward_normalization=False, \n",
    "    advantage_normalization = False,\n",
    "    value_clip = False,\n",
    "    deterministic_eval=True,\n",
    "    action_scaling= True,\n",
    "    action_bound_method= \"tanh\",\n",
    "    ent_coef = 1e-3,\n",
    "    vf_coef = 5e-2,\n",
    "    max_grad_norm = 1.,\n",
    "    gae_lambda=0.9, \n",
    "    recompute_advantage=False,\n",
    ")\n",
    "\n",
    "policy2 = PGPolicy(model=actor, optim=optim, dist_fn=dist, action_space=train_env.action_space,\n",
    "        discount_factor=0.9,reward_normalization=False, deterministic_eval=True,\n",
    "        observation_space=train_env.observation_space, action_scaling=True, action_bound_method = \"tanh\",\n",
    "    )\n",
    "\n",
    "policy3 = IndpPGPolicy(model=actor, optim=optim, dist_fn=dist, action_space=train_env.action_space,\n",
    "        discount_factor=0.9,reward_normalization=False, deterministic_eval=True,\n",
    "        observation_space=train_env.observation_space, action_scaling=True, action_bound_method = \"tanh\",\n",
    "    )\n",
    "\n",
    "\n",
    "#######################################################################################################\n",
    "####### Collectors ####################################################################################\n",
    "#######################################################################################################\n",
    "#train_collector = Collector(policy=policy, env=train_env, buffer=VectorReplayBuffer(args.buffer_size, len(train_env)))\n",
    "train_collector = Collector(policy=policy3, env=train_env, buffer=VectorReplayBuffer(20000, 1))\n",
    "test_collector = Collector(policy=policy3, env=test_env)\n",
    "train_collector.reset()\n",
    "test_collector.reset()\n",
    "\n",
    "#######################################################################################################\n",
    "####### Trainer #######################################################################################\n",
    "#######################################################################################################\n",
    "trainer = OnpolicyTrainer(\n",
    "    policy=policy3,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=2,\n",
    "    step_per_epoch=100,\n",
    "    repeat_per_collect=1,\n",
    "    episode_per_test=1,\n",
    "    batch_size=16,\n",
    "    step_per_collect=32,\n",
    "    #episode_per_collect=args.episode_per_collect,\n",
    "    show_progress=True,\n",
    "    #stop_fn=lambda mean_reward: mean_reward >= args.reward_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.11111111, dtype=float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "        \"\"\"Perform one training step.\"\"\"\n",
    "        assert trainer.episode_per_test is not None\n",
    "        assert trainer.train_collector is not None\n",
    "        stop_fn_flag = False\n",
    "\n",
    "        #what does train_fn do?\n",
    "        if trainer.train_fn:\n",
    "            trainer.train_fn(trainer.epoch, trainer.env_step)\n",
    "\n",
    "        #collect either step_per_collect or episode_per_collect results\n",
    "        result = trainer.train_collector.collect(n_step=trainer.step_per_collect, n_episode=trainer.episode_per_collect)\n",
    "\n",
    "        #transforms rewards\n",
    "        if result[\"n/ep\"] > 0 and trainer.reward_metric:\n",
    "            rew = trainer.reward_metric(result[\"rews\"]) #seems to apply the reward_metric ontop of the collected rewards\n",
    "            result.update(rews=rew, rew=rew.mean(), rew_std=rew.std())\n",
    "\n",
    "        trainer.env_step += int(result[\"n/st\"]) #n/st measures the total number of steps taken in the environment\n",
    "        trainer.logger.log_train_data(result, trainer.env_step) #logs training data\n",
    "        trainer.last_rew = result[\"rew\"] if result[\"n/ep\"] > 0 else trainer.last_rew #updates last reward if a whole episode was played\n",
    "        trainer.last_len = result[\"len\"] if result[\"n/ep\"] > 0 else trainer.last_len #updates last length if a whole episode was played\n",
    "\n",
    "        #updated printed statistics of the call to train_step\n",
    "        data = {\n",
    "            \"env_step\": str(trainer.env_step),\n",
    "            \"rew\": f\"{trainer.last_rew:.2f}\",\n",
    "            \"len\": str(int(trainer.last_len)),\n",
    "            \"n/ep\": str(int(result[\"n/ep\"])),\n",
    "            \"n/st\": str(int(result[\"n/st\"])),\n",
    "        }\n",
    "\n",
    "        if result[\"n/ep\"] > 0:\n",
    "            #if stop_fn returns true on training data we perform a test and check if stop_fn also\n",
    "            #returns true on testing data. If so, we stop everything by setting stop_fn_flag = True\n",
    "            if trainer.test_in_train and trainer.stop_fn and trainer.stop_fn(result[\"rew\"]):\n",
    "                assert trainer.test_collector is not None\n",
    "                test_result = test_episode(\n",
    "                    trainer.policy, trainer.test_collector, trainer.test_fn, trainer.epoch,\n",
    "                    trainer.episode_per_test, trainer.logger, trainer.env_step\n",
    "                )\n",
    "\n",
    "                if trainer.stop_fn(test_result[\"rew\"]):\n",
    "                    stop_fn_flag = True\n",
    "                    trainer.best_reward = test_result[\"rew\"]\n",
    "                    trainer.best_reward_std = test_result[\"rew_std\"]\n",
    "                else:\n",
    "                    trainer.policy.train()\n",
    "\n",
    "        # returns results of training, not of testing!\n",
    "        return data, result, stop_fn_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand policy_update_fn\n",
    "def policy_update_fn(data, result):\n",
    "        \"\"\"Perform one on-policy update.\"\"\"\n",
    "        assert trainer.train_collector is not None\n",
    "        \n",
    "        losses = trainer.policy.update(\n",
    "            0,\n",
    "            trainer.train_collector.buffer,\n",
    "            batch_size=trainer.batch_size,\n",
    "            repeat=trainer.repeat_per_collect,\n",
    "        )\n",
    "        trainer.train_collector.reset_buffer(keep_statistics=True)\n",
    "        step = max([1] + [len(v) for v in losses.values() if isinstance(v, list)])\n",
    "        trainer.gradient_step += step\n",
    "        trainer.log_update_data(data, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:  32%|███▏      | 32/100 [00:02<00:05, 12.60it/s, env_step=32, len=0, loss=-0.001, n/ep=0, n/st=32, rew=0.00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env_step': '32', 'rew': '0.00', 'len': '0', 'n/ep': '0', 'n/st': '32'}\n",
      "{'n/ep': 0, 'n/st': 32, 'rews': array([], dtype=float64), 'lens': array([], dtype=int64), 'idxs': array([], dtype=int64), 'rew': 0, 'len': 0, 'rew_std': 0, 'len_std': 0}\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:  96%|█████████▌| 96/100 [00:03<00:00, 42.93it/s, env_step=64, len=0, loss=-0.001, n/ep=0, n/st=32, rew=0.00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env_step': '64', 'rew': '0.00', 'len': '0', 'n/ep': '0', 'n/st': '32'}\n",
      "{'n/ep': 0, 'n/st': 32, 'rews': array([], dtype=float64), 'lens': array([], dtype=int64), 'idxs': array([], dtype=int64), 'rew': 0, 'len': 0, 'rew_std': 0, 'len_std': 0}\n",
      "False\n",
      "{'env_step': '96', 'rew': '0.00', 'len': '0', 'n/ep': '0', 'n/st': '32'}\n",
      "{'n/ep': 0, 'n/st': 32, 'rews': array([], dtype=float64), 'lens': array([], dtype=int64), 'idxs': array([], dtype=int64), 'rew': 0, 'len': 0, 'rew_std': 0, 'len_std': 0}\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 128it [00:03, 39.73it/s, env_step=128, len=0, loss=-0.000, n/ep=0, n/st=32, rew=0.00]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env_step': '128', 'rew': '0.00', 'len': '0', 'n/ep': '0', 'n/st': '32'}\n",
      "{'n/ep': 0, 'n/st': 32, 'rews': array([], dtype=float64), 'lens': array([], dtype=int64), 'idxs': array([], dtype=int64), 'rew': 0, 'len': 0, 'rew_std': 0, 'len_std': 0}\n",
      "False\n",
      "Epoch #1: test_reward: -14.351458 ± 13.403692, best_reward: -9.832229 ± 9.662032 in #0\n",
      "return\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2:  32%|███▏      | 32/100 [00:00<00:00, 219.27it/s, env_step=160, len=0, loss=-0.000, n/ep=0, n/st=32, rew=0.00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env_step': '160', 'rew': '0.00', 'len': '0', 'n/ep': '0', 'n/st': '32'}\n",
      "{'n/ep': 0, 'n/st': 32, 'rews': array([], dtype=float64), 'lens': array([], dtype=int64), 'idxs': array([], dtype=int64), 'rew': 0, 'len': 0, 'rew_std': 0, 'len_std': 0}\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2:  64%|██████▍   | 64/100 [00:00<00:00, 193.74it/s, env_step=160, len=0, loss=-0.000, n/ep=0, n/st=32, rew=0.00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env_step': '192', 'rew': '0.00', 'len': '0', 'n/ep': '0', 'n/st': '32'}\n",
      "{'n/ep': 0, 'n/st': 32, 'rews': array([], dtype=float64), 'lens': array([], dtype=int64), 'idxs': array([], dtype=int64), 'rew': 0, 'len': 0, 'rew_std': 0, 'len_std': 0}\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2:  96%|█████████▌| 96/100 [00:00<00:00, 187.74it/s, env_step=192, len=0, loss=-0.000, n/ep=0, n/st=32, rew=0.00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env_step': '224', 'rew': '0.00', 'len': '0', 'n/ep': '0', 'n/st': '32'}\n",
      "{'n/ep': 0, 'n/st': 32, 'rews': array([], dtype=float64), 'lens': array([], dtype=int64), 'idxs': array([], dtype=int64), 'rew': 0, 'len': 0, 'rew_std': 0, 'len_std': 0}\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 128it [00:00, 185.01it/s, env_step=224, len=0, loss=-0.000, n/ep=0, n/st=32, rew=0.00]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env_step': '256', 'rew': '0.00', 'len': '0', 'n/ep': '0', 'n/st': '32'}\n",
      "{'n/ep': 0, 'n/st': 32, 'rews': array([], dtype=float64), 'lens': array([], dtype=int64), 'idxs': array([], dtype=int64), 'rew': 0, 'len': 0, 'rew_std': 0, 'len_std': 0}\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 128it [00:00, 180.18it/s, env_step=256, len=0, loss=-0.000, n/ep=0, n/st=32, rew=0.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -14.213970 ± 13.288224, best_reward: -9.832229 ± 9.662032 in #0\n",
      "return\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "step_per_epoch = 100\n",
    "global_iter_num = 1 #counts what? -> total number of iterations with the environment\n",
    "stop_fn_flag = False #should be set to true if stop_fn otputs true\n",
    "show_progress = True #flag indicating the visualization of a progress bar\n",
    "trainer.reset()\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    \"\"\"Perform one epoch (both train and eval).\"\"\"\n",
    "    global_iter_num += 1\n",
    "    trainer.epoch+=1\n",
    "    if global_iter_num > 1:\n",
    "        # exit flag 1, when stop_fn succeeds in train_step or test_step\n",
    "        if stop_fn_flag:\n",
    "            print(\"stop iteration due to stop_fn_flag\")\n",
    "            break\n",
    "    \n",
    "    policy.train() #set policy in train mode\n",
    "    epoch_stat = dict() #dictionary to save epoch statistics\n",
    "\n",
    "\n",
    "    # perform n step_per_epoch\n",
    "    progress = tqdm if show_progress else DummyTqdm\n",
    "    with progress(total=step_per_epoch, desc=f\"Epoch #{epoch}\") as t:\n",
    "        while t.n < t.total and not stop_fn_flag:\n",
    "            data = dict()\n",
    "            result = dict()\n",
    "            #********************************************************************************************************************\n",
    "            #PERFORM A COLLECT STEP *********************************************************************************************\n",
    "            #********************************************************************************************************************\n",
    "            if trainer.train_collector is not None:\n",
    "                data, result, stop_fn_flag = train_step()\n",
    "                t.update(result[\"n/st\"]) #updates the counter t with the total number of steps taken in the last train_step update!\n",
    "                if trainer.stop_fn_flag:\n",
    "                    t.set_postfix(**data) #sets print output of progress bar\n",
    "                    break #breaks while loop in case stop_fn is true\n",
    "            #********************************************************************************************************************\n",
    "            #********************************************************************************************************************\n",
    "            #********************************************************************************************************************\n",
    "            else:\n",
    "                print(\"WARNING: o train_collector was found\")\n",
    "                assert trainer.buffer, \"No train_collector or buffer specified\"\n",
    "                result[\"n/ep\"] = len(trainer.buffer)\n",
    "                result[\"n/st\"] = int(trainer.gradient_step)\n",
    "                t.update()\n",
    "\n",
    "            #********************************************************************************************************************\n",
    "            #PERFORM A POLICY UPDATE ********************************************************************************************\n",
    "            #********************************************************************************************************************\n",
    "            #updates the policy with the collected data and results \n",
    "            print(data)\n",
    "            print(result)\n",
    "            print(stop_fn_flag)\n",
    "            policy_update_fn(data, result)\n",
    "            t.set_postfix(**data)\n",
    "            #********************************************************************************************************************\n",
    "            #********************************************************************************************************************\n",
    "            #********************************************************************************************************************\n",
    "\n",
    "\n",
    "    # for offline RL\n",
    "    if trainer.train_collector is None:\n",
    "        trainer.env_step = trainer.gradient_step * trainer.batch_size\n",
    "    \n",
    "    # loggs training data if stop_fn_flag is not true jet, hence logs testing stats after each epoch\n",
    "    if not trainer.stop_fn_flag:\n",
    "        trainer.logger.save_data(trainer.epoch, trainer.env_step, trainer.gradient_step, trainer.save_checkpoint_fn)\n",
    "        #********************************************************************************************************************\n",
    "        # PERFORM A TESTING STEP ********************************************************************************************\n",
    "        #********************************************************************************************************************\n",
    "        if trainer.test_collector is not None:\n",
    "            test_stat, trainer.stop_fn_flag = trainer.test_step()\n",
    "            if not trainer.is_run: #if trainer isn't running \n",
    "                epoch_stat.update(test_stat)\n",
    "        #********************************************************************************************************************\n",
    "        #********************************************************************************************************************\n",
    "        #********************************************************************************************************************\n",
    "        \n",
    "\n",
    "    # if trainer is not running -> meaning logs training stats after each epoch if trainer has finished \n",
    "    if not trainer.is_run:\n",
    "        epoch_stat.update({k: v.get() for k, v in trainer.stat.items()})\n",
    "        epoch_stat[\"gradient_step\"] = trainer.gradient_step\n",
    "        epoch_stat.update(\n",
    "            {\n",
    "                \"env_step\": trainer.env_step,\n",
    "                \"rew\": trainer.last_rew,\n",
    "                \"len\": int(trainer.last_len),\n",
    "                \"n/ep\": int(result[\"n/ep\"]),\n",
    "                \"n/st\": int(result[\"n/st\"]),\n",
    "            }\n",
    "        )\n",
    "        info = gather_info(\n",
    "            trainer.start_time, trainer.train_collector, trainer.test_collector,\n",
    "            trainer.best_reward, trainer.best_reward_std\n",
    "        )\n",
    "        print(\"return\")\n",
    "        #TODO: do something with the info here -> or just return it\n",
    "\n",
    "    else:\n",
    "        print(\"no return cause trainer is still running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_env\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_env' is not defined"
     ]
    }
   ],
   "source": [
    "train_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlb2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
